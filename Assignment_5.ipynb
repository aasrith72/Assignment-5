{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdM39fOhVV+kMimrDxlm39",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aasrith72/Assignment-5/blob/main/Assignment_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4D7jo7kolr-9"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets faiss-cpu\n",
        "# !pip       : Shell command to install Python packages.\n",
        "# -q         : Quiet mode (suppresses output).\n",
        "# transformers: Python library for pre-trained models (Hugging Face).\n",
        "# datasets   : Python library for loading datasets (Hugging Face).\n",
        "# faiss-cpu  : Facebook AI Similarity Search library (for vector operations) on CPU.\n",
        "\n",
        "sample_text=\"\"\"\n",
        "# sample_text: The document/text that serves as our knowledge base.\n",
        "Albert Einstein was a theoretical physicist who developed the theory of relativity,\n",
        " one of the two pillars of modern physics (alongside quantum mechanics). His work is also\n",
        " known for its influence on the philosophy of science. He is best known to the general public\n",
        " for his mass-energy equivalence formula E = mc².\n",
        "\"\"\"\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "# AutoTokenizer: Automatically loads the correct tokenizer for a model.\n",
        "# AutoModel   : Automatically loads the correct model architecture.\n",
        "\n",
        "import torch\n",
        "# torch      : PyTorch library for deep learning operations (tensors, neural networks).\n",
        "\n",
        "import numpy as np\n",
        "# numpy      : Library for numerical operations, especially array manipulation.\n",
        "# np         : Common alias for numpy.\n",
        "\n",
        "model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "# model_name : Identifier for the specific pre-trained embedding model.\n",
        "\n",
        "tokenizer=AutoTokenizer.from_pretrained(model_name)\n",
        "# tokenizer  : The tool that converts text into numerical tokens for the model.\n",
        "\n",
        "model=AutoModel.from_pretrained(model_name)\n",
        "# model      : The pre-trained neural network that will generate embeddings.\n",
        "\n",
        "def get_embeddings(text):\n",
        "# get_embeddings: Function to convert text into a numerical vector (embedding).\n",
        "\n",
        "tokens=tokenizer(text,return_tensors='pt', truncation=True, padding=True)\n",
        "# tokens     : Tokenized input format for the model.\n",
        "# return_tensors='pt': Specifies PyTorch tensors as output format.\n",
        "# truncation=True: Cuts text if it's too long.\n",
        "# padding=True : Adds padding to short texts to match length.\n",
        "\n",
        "with torch.no_grad():\n",
        "# torch.no_grad(): Disables gradient calculation, useful for inference (not training) to save memory.\n",
        "\n",
        "outputs=model(**tokens)\n",
        "# outputs    : The raw output from the neural network model.\n",
        "\n",
        "return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "# .last_hidden_state: Accesses the contextual embeddings for each token from the model's output.\n",
        "# .mean(dim=1) : Calculates the average of token embeddings to get a single text embedding.\n",
        "# .squeeze()   : Removes single-dimensional entries from the tensor.\n",
        "# .numpy()     : Converts the PyTorch tensor to a NumPy array.\n",
        "\n",
        "import faiss\n",
        "# faiss      : Library for efficient similarity search on vectors.\n",
        "\n",
        "chunks=[sample_text]\n",
        "# chunks     : A list of text segments (our knowledge base documents).\n",
        "\n",
        "embeddings=[get_embeddings(chunk) for chunk in chunks]\n",
        "# embeddings : A list of numerical vector representations for each text chunk.\n",
        "# list comprehension: A concise way to create lists.\n",
        "\n",
        "dim=len(embeddings[0])\n",
        "# dim        : The dimensionality (length) of the embedding vectors.\n",
        "\n",
        "index=faiss.IndexFlatL2(dim)\n",
        "# index      : The FAISS index structure for storing and searching vectors.\n",
        "# faiss.IndexFlatL2: Creates a flat index using L2 (Euclidean) distance for similarity.\n",
        "\n",
        "index.add(np.array(embeddings))\n",
        "# .add       : Method to add embedding vectors to the FAISS index.\n",
        "# np.array() : Converts a Python list into a NumPy array.\n",
        "\n",
        "from transformers import pipeline\n",
        "# pipeline   : A high-level helper function from transformers to simplify model usage for common tasks.\n",
        "\n",
        "qa_pipeline =pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
        "# qa_pipeline: Our text generation pipeline.\n",
        "# \"text2text-generation\": Specifies the task type for the pipeline.\n",
        "# model=\"google/flan-t5-small\": The specific LLM (Large Language Model) to use for generation.\n",
        "\n",
        "def retrive_and_answer(query,top_k=1):\n",
        "  # retrive_and_answer: Function that orchestrates the RAG process (retrieval + answer generation).\n",
        "  # query      : The user's input question.\n",
        "  # top_k      : The number of most relevant documents/chunks to retrieve.\n",
        "\n",
        "  query_embedding = get_embeddings(query).reshape(1,-1)\n",
        "  # query_embedding: The numerical vector representation of the user's query.\n",
        "  # .reshape(1,-1): Reshapes the vector into a 2D array suitable for FAISS search.\n",
        "\n",
        "  distances, indices = index.search(query_embedding,top_k)\n",
        "  # distances  : The similarity scores (L2 distance) of retrieved items.\n",
        "  # indices    : The positions/IDs of the retrieved chunks in the original 'chunks' list.\n",
        "  # .search    : Method to perform the similarity search on the FAISS index.\n",
        "\n",
        "  retrived_texts = [chunks[i] for i in indices[0].tolist()]\n",
        "  # retrived_texts: The actual text content of the chunks found by the search.\n",
        "  # .tolist()  : Converts a NumPy array to a Python list.\n",
        "\n",
        "  context=\" \".join(retrived_texts)\n",
        "  # context    : The combined text of all retrieved chunks, forming the information for the LLM.\n",
        "  # .join()    : String method to concatenate elements of a list into a single string.\n",
        "\n",
        "  prompt=f'Context: {context} \\nQuestion :{query} \\nAnswer: '\n",
        "  # prompt     : The final, augmented input string given to the LLM for generating an answer.\n",
        "  # f''        : F-string for easy embedding of variables into a string.\n",
        "  # \\n         : Newline character.\n",
        "\n",
        "  result = qa_pipeline([prompt],max_length=100,do_sample=False)\n",
        "  # result     : The output generated by the LLM.\n",
        "  # max_length=100: Sets the maximum length of the generated answer.\n",
        "  # do_sample=False: Ensures deterministic (non-random) output from the LLM.\n",
        "\n",
        "  print(\"Pipeline Result:\", result) # Add this line to print the result\n",
        "  # print      : Standard Python print function.\n",
        "\n",
        "  return result[0]['generated_text']\n",
        "  # ['generated_text']: Key to access the actual generated text from the pipeline's output.\n",
        "\n",
        "Question=\"What is Albert Einstein known for?\"\n",
        "# Question   : The specific query being asked.\n",
        "\n",
        "Answer=retrive_and_answer(Question)\n",
        "# Answer     : The final answer produced by the RAG system.\n",
        "\n",
        "print(\"Q:\",Question)\n",
        "print(\"A:\",Answer)"
      ]
    }
  ]
}